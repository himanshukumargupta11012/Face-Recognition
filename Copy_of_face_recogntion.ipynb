{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zPhxJj99LDln"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import  numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, models\n",
        "# import gdown\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)\n",
        "\n",
        "# face_localization_model = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSiTG7ZbLDlq"
      },
      "source": [
        "Upload your kaggle API key to download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eoh26dp5LDlr",
        "outputId": "aabf7aaf-8e79-49f9-b6aa-c065a1f36a23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: /home/ubuntu/.venv/bin/kaggle: /home/ubuntu/tihan/tihan_eir/venv_tihan/bin/python3: bad interpreter: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle\n",
        "\n",
        "!kaggle datasets download -d stoicstatic/face-recognition-dataset\n",
        "\n",
        "if not os.path.exists(\"Extracted Faces\"):\n",
        "    !unzip -q face-recognition-dataset.zip \"Extracted Faces/*\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIko8XlBLDls"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = torch.load(\"face_data.pth\")\n",
        "\n",
        "train_dataset = data[\"train\"]\n",
        "test_dataset = data[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX8U-HbELDlu"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model creation\n",
        "additional_layer = nn.Linear(2048, 256)\n",
        "\n",
        "resnet = models.resnet50(weights=\"DEFAULT\")\n",
        "encoder_layers = list(resnet.children())[:-1]\n",
        "encoder_model = torch.nn.Sequential(*encoder_layers)\n",
        "\n",
        "for param in encoder_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model = nn.Sequential(encoder_model, nn.Flatten(), nn.Linear(2048, 256)).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "t-DsQRohLDlv"
      },
      "outputs": [],
      "source": [
        "def train(model, lr, n_epochs, dataloader):\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr, weight_decay=0.0001, momentum=0.9)\n",
        "\n",
        "    for i in range(n_epochs):\n",
        "        epoch_loss = 0\n",
        "        print(accuracy(model, dataloader)[0], accuracy(model, test_dataloader)[0])\n",
        "        \n",
        "        for input, label in dataloader:\n",
        "            input = input.to(device)\n",
        "            label = label.to(device)\n",
        "            combined_input = torch.cat((input[:, 0:3], input[:, 3:]), dim=0)\n",
        "            output = model(combined_input)\n",
        "            val1, val2 = torch.split(output, output.shape[0] // 2)\n",
        "\n",
        "            distance = nn.functional.pairwise_distance(val1, val2, keepdim=True)\n",
        "            # print(distance)\n",
        "            margin = 0.5\n",
        "            loss = torch.mean((label) * torch.pow(distance, 2) + (1 - label) * torch.pow(torch.clamp(margin - distance, min=0.0), 2))\n",
        "\n",
        "            epoch_loss += loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"loss: \", epoch_loss.item() / len(dataloader))\n",
        "\n",
        "    print(accuracy(model, dataloader)[0], accuracy(model, test_dataloader)[0])\n",
        "\n",
        "# model = SiameseCNN().to(device)\n",
        "\n",
        "def accuracy(model, dataloader):\n",
        "    model.eval()\n",
        "    confusion_matrix = np.zeros((2, 2))\n",
        "\n",
        "\n",
        "    for input, label in dataloader:\n",
        "        input = input.to(device)\n",
        "        label = label.to(device)\n",
        "        combined_input = torch.cat((input[:, 0:3], input[:, 3:]), dim=0)\n",
        "        output = model(combined_input)\n",
        "        val1, val2 = torch.split(output, output.shape[0] // 2)\n",
        "\n",
        "        distance = nn.functional.pairwise_distance(val1, val2, keepdim=True)\n",
        "        margin = 0.5\n",
        "        pred = (distance < margin).int()\n",
        "\n",
        "        confusion_matrix[0][0] += torch.sum((pred == 1) & (label == 1)).item()\n",
        "        confusion_matrix[0][1] += torch.sum((pred == 1) & (label == 0)).item()\n",
        "        confusion_matrix[1][0] += torch.sum((pred == 0) & (label == 1)).item()\n",
        "        confusion_matrix[1][1] += torch.sum((pred == 0) & (label == 0)).item()\n",
        "\n",
        "        correct_ratio = np.trace(confusion_matrix) / np.sum(confusion_matrix)\n",
        "\n",
        "    return correct_ratio, confusion_matrix / np.sum(confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5005 0.49925\n",
            "loss:  0.1222837890625\n",
            "0.5098 0.51525\n",
            "loss:  0.0886573974609375\n",
            "0.507675 0.507\n",
            "loss:  0.07769096069335937\n",
            "0.502125 0.504\n",
            "loss:  0.07721958618164063\n",
            "0.500775 0.5015\n",
            "loss:  0.07810003662109374\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mface_recognition_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[48], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, lr, n_epochs, dataloader)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m      5\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m], accuracy(model, test_dataloader)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m, label \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mto(device)\n",
            "Cell \u001b[0;32mIn[48], line 48\u001b[0m, in \u001b[0;36maccuracy\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     45\u001b[0m margin \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     46\u001b[0m pred \u001b[38;5;241m=\u001b[39m (distance \u001b[38;5;241m<\u001b[39m margin)\u001b[38;5;241m.\u001b[39mint()\n\u001b[0;32m---> 48\u001b[0m confusion_matrix[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m confusion_matrix[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum((pred \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m (label \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     50\u001b[0m confusion_matrix[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum((pred \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (label \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mitem()\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train(model, .01, 20, train_dataloader)\n",
        "torch.save(model, \"face_recognition_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0.812, array([[0.391, 0.079],\n",
            "       [0.109, 0.421]]))\n"
          ]
        }
      ],
      "source": [
        "# model = torch.load(\"face_recognition_model3.pth\", map_location=device)\n",
        "# model.eval()\n",
        "\n",
        "print(accuracy(model, test_dataloader))\n",
        "# print(accuracy(model, train_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kN176XX1QTVF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[ WARN:0@1089.472] global loadsave.cpp:248 findDecoder imread_('test1.jpg'): can't open/read file: check file path/integrity\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'face_localization_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[0;32m---> 16\u001b[0m img1 \u001b[38;5;241m=\u001b[39m \u001b[43mtestImgPreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest1.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m img2 \u001b[38;5;241m=\u001b[39m testImgPreprocess(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest2.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m img3 \u001b[38;5;241m=\u001b[39m testImgPreprocess(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest3.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[9], line 3\u001b[0m, in \u001b[0;36mtestImgPreprocess\u001b[0;34m(img_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtestImgPreprocess\u001b[39m(img_path):\n\u001b[1;32m      2\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(img_path,  cv2\u001b[38;5;241m.\u001b[39mIMREAD_GRAYSCALE)\n\u001b[0;32m----> 3\u001b[0m     cordinates \u001b[38;5;241m=\u001b[39m \u001b[43mface_localization_model\u001b[49m\u001b[38;5;241m.\u001b[39mdetectMultiScale(img, scaleFactor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.1\u001b[39m, minNeighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(cordinates)\n\u001b[1;32m      5\u001b[0m     x, y, w, h \u001b[38;5;241m=\u001b[39m cordinates[\u001b[38;5;241m0\u001b[39m]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'face_localization_model' is not defined"
          ]
        }
      ],
      "source": [
        "def testImgPreprocess(img_path):\n",
        "    img = cv2.imread(img_path,  cv2.IMREAD_GRAYSCALE)\n",
        "    cordinates = face_localization_model.detectMultiScale(img, scaleFactor=1.1, minNeighbors=5)\n",
        "    print(cordinates)\n",
        "    x, y, w, h = cordinates[0]\n",
        "    img = img[x:x+w, y:y+h]\n",
        "\n",
        "    img = img / 255\n",
        "    img = cv2.resize(img, (128, 128))\n",
        "\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "    return img\n",
        "\n",
        "img1 = testImgPreprocess(\"test1.jpg\")\n",
        "img2 = testImgPreprocess(\"test2.jpg\")\n",
        "img3 = testImgPreprocess(\"test3.jpg\")\n",
        "\n",
        "accuracy([[img2, img3]], [0])\n",
        "\n",
        "dummy_input = torch.rand((1, 1, 128, 128)).to(device)\n",
        "torch.onnx.export(model, dummy_input, \"face_detection.onnx\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
