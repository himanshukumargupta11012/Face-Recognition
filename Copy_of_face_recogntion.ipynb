{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zPhxJj99LDln"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import  numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, models\n",
        "# import gdown\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)\n",
        "\n",
        "# face_localization_model = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSiTG7ZbLDlq"
      },
      "source": [
        "Upload your kaggle API key to download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eoh26dp5LDlr",
        "outputId": "aabf7aaf-8e79-49f9-b6aa-c065a1f36a23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/stoicstatic/face-recognition-dataset\n",
            "License(s): CC0-1.0\n",
            "face-recognition-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle\n",
        "\n",
        "!kaggle datasets download -d stoicstatic/face-recognition-dataset\n",
        "\n",
        "if not os.path.exists(\"Extracted Faces\"):\n",
        "    !unzip -q face-recognition-dataset.zip \"Extracted Faces/*\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIko8XlBLDls"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ],
      "source": [
        "data = torch.load(\"face_data2.pth\")\n",
        "\n",
        "train_dataset = data[\"train\"]\n",
        "test_dataset = data[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX8U-HbELDlu"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model creation\n",
        "additional_layer = nn.Linear(2048, 256)\n",
        "\n",
        "resnet = models.resnet50(weights=\"DEFAULT\")\n",
        "encoder_layers = list(resnet.children())[:-1]\n",
        "encoder_model = torch.nn.Sequential(*encoder_layers)\n",
        "\n",
        "for param in encoder_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model = nn.Sequential(encoder_model, nn.Flatten(), nn.Linear(2048, 256)).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "t-DsQRohLDlv"
      },
      "outputs": [],
      "source": [
        "def train(model, lr, n_epochs, dataloader):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "\n",
        "    for i in range(n_epochs):\n",
        "        epoch_loss = 0\n",
        "        print(accuracy(model, dataloader)[0], accuracy(model, test_dataloader)[0])\n",
        "        \n",
        "        for input, label in dataloader:\n",
        "            input = input.to(device)\n",
        "            label = label.to(device)\n",
        "            combined_input = torch.cat((input[:, 0:3], input[:, 3:]), dim=0)\n",
        "            output = model(combined_input)\n",
        "            val1, val2 = torch.split(output, output.shape[0] // 2)\n",
        "\n",
        "            distance = nn.functional.pairwise_distance(val1, val2, keepdim=True)\n",
        "            # print(val1, val2)\n",
        "            # print(distance)\n",
        "            # break\n",
        "            margin = 2\n",
        "            loss = torch.mean((label) * distance**2 + (1 - label) * torch.clamp(margin - distance, min=0.0)**2)\n",
        "\n",
        "            epoch_loss += loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"loss: \", epoch_loss.item() / len(dataloader))\n",
        "\n",
        "    # print(accuracy(model, dataloader)[0], accuracy(model, test_dataloader)[0])\n",
        "        print(accuracy(model, dataloader)[1])\n",
        "\n",
        "# model = SiameseCNN().to(device)\n",
        "\n",
        "def accuracy(model, dataloader):\n",
        "    model.eval()\n",
        "    confusion_matrix = np.zeros((2, 2))\n",
        "\n",
        "\n",
        "    for input, label in dataloader:\n",
        "        input = input.to(device)\n",
        "        label = label.to(device)\n",
        "        combined_input = torch.cat((input[:, 0:3], input[:, 3:]), dim=0)\n",
        "        output = model(combined_input)\n",
        "        val1, val2 = torch.split(output, output.shape[0] // 2)\n",
        "\n",
        "        distance = nn.functional.pairwise_distance(val1, val2, keepdim=True)\n",
        "        margin = 2\n",
        "        pred = (distance < margin).int()\n",
        "\n",
        "        confusion_matrix[0][0] += torch.sum((pred == 1) & (label == 1)).item()\n",
        "        confusion_matrix[0][1] += torch.sum((pred == 1) & (label == 0)).item()\n",
        "        confusion_matrix[1][0] += torch.sum((pred == 0) & (label == 1)).item()\n",
        "        confusion_matrix[1][1] += torch.sum((pred == 0) & (label == 0)).item()\n",
        "\n",
        "        correct_ratio = np.trace(confusion_matrix) / np.sum(confusion_matrix)\n",
        "\n",
        "    return correct_ratio, confusion_matrix / np.sum(confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5165 0.5204081632653061\n",
            "loss:  1.23926123046875\n",
            "[[0.4865 0.4845]\n",
            " [0.0135 0.0155]]\n",
            "0.50125 0.5\n",
            "loss:  1.211468994140625\n",
            "[[0.47575 0.47925]\n",
            " [0.02425 0.02075]]\n",
            "0.49975 0.5025510204081632\n",
            "loss:  1.382403076171875\n",
            "[[0.47175 0.47225]\n",
            " [0.02825 0.02775]]\n",
            "0.502 0.4872448979591837\n",
            "loss:  1.2913807373046875\n",
            "[[0.466125 0.470875]\n",
            " [0.033875 0.029125]]\n",
            "0.50475 0.5\n",
            "loss:  1.206465576171875\n",
            "[[0.47675 0.47425]\n",
            " [0.02325 0.02575]]\n",
            "0.5015 0.5051020408163265\n",
            "loss:  1.242101806640625\n",
            "[[0.4775 0.4775]\n",
            " [0.0225 0.0225]]\n",
            "0.5 0.5\n",
            "loss:  1.21904248046875\n",
            "[[0.468875 0.464125]\n",
            " [0.031125 0.035875]]\n",
            "0.50075 0.49744897959183676\n",
            "loss:  1.5067196044921876\n",
            "[[0.4765 0.4755]\n",
            " [0.0235 0.0245]]\n",
            "0.49675 0.5076530612244898\n",
            "loss:  1.4090745849609374\n",
            "[[0.462625 0.463375]\n",
            " [0.037375 0.036625]]\n",
            "0.501 0.5051020408163265\n",
            "loss:  1.386261962890625\n",
            "[[0.459875 0.463125]\n",
            " [0.040125 0.036875]]\n"
          ]
        }
      ],
      "source": [
        "train(model, .001, 10, train_dataloader)\n",
        "torch.save(model, \"face_recognition_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0.5, array([[0.5, 0.5],\n",
            "       [0. , 0. ]]))\n"
          ]
        }
      ],
      "source": [
        "# model = torch.load(\"face_recognition_model3.pth\", map_location=device)\n",
        "# model.eval()\n",
        "\n",
        "print(accuracy(model, test_dataloader))\n",
        "# print(accuracy(model, train_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kN176XX1QTVF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[ WARN:0@70.026] global loadsave.cpp:248 findDecoder imread_('test1.jpg'): can't open/read file: check file path/integrity\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'face_localization_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[0;32m---> 16\u001b[0m img1 \u001b[38;5;241m=\u001b[39m \u001b[43mtestImgPreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest1.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m img2 \u001b[38;5;241m=\u001b[39m testImgPreprocess(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest2.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m img3 \u001b[38;5;241m=\u001b[39m testImgPreprocess(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest3.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[10], line 3\u001b[0m, in \u001b[0;36mtestImgPreprocess\u001b[0;34m(img_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtestImgPreprocess\u001b[39m(img_path):\n\u001b[1;32m      2\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(img_path,  cv2\u001b[38;5;241m.\u001b[39mIMREAD_GRAYSCALE)\n\u001b[0;32m----> 3\u001b[0m     cordinates \u001b[38;5;241m=\u001b[39m \u001b[43mface_localization_model\u001b[49m\u001b[38;5;241m.\u001b[39mdetectMultiScale(img, scaleFactor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.1\u001b[39m, minNeighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(cordinates)\n\u001b[1;32m      5\u001b[0m     x, y, w, h \u001b[38;5;241m=\u001b[39m cordinates[\u001b[38;5;241m0\u001b[39m]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'face_localization_model' is not defined"
          ]
        }
      ],
      "source": [
        "def testImgPreprocess(img_path):\n",
        "    img = cv2.imread(img_path,  cv2.IMREAD_GRAYSCALE)\n",
        "    cordinates = face_localization_model.detectMultiScale(img, scaleFactor=1.1, minNeighbors=5)\n",
        "    print(cordinates)\n",
        "    x, y, w, h = cordinates[0]\n",
        "    img = img[x:x+w, y:y+h]\n",
        "\n",
        "    img = img / 255\n",
        "    img = cv2.resize(img, (128, 128))\n",
        "\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "    return img\n",
        "\n",
        "img1 = testImgPreprocess(\"test1.jpg\")\n",
        "img2 = testImgPreprocess(\"test2.jpg\")\n",
        "img3 = testImgPreprocess(\"test3.jpg\")\n",
        "\n",
        "accuracy([[img2, img3]], [0])\n",
        "\n",
        "dummy_input = torch.rand((1, 1, 128, 128)).to(device)\n",
        "torch.onnx.export(model, dummy_input, \"face_detection.onnx\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
